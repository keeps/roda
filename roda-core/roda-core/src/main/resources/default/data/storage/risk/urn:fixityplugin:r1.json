{
  "id": "urn:fixityplugin:r1",
  "name": "File(s) corrupted due to hardware malfunction or human intervention",
  "description": "\"Fixity\", in the preservation sense, means the assurance that a digital file has remained unchanged, i.e. fixed. Fixity doesn’t just apply to files, but to any digital object that has a series of bits inside it where that \"bitstream\" needs to be kept intact with the knowledge that it hasn’t changed. Fixity could be applied to images or video inside an audiovisual object, to individual files within a zip, to metadata inside an XML structure, to records in a database, or to objects in an object store. However, files are currently the most common way of storing digital materials and fixity of files can established and monitored through the use of checksums.\n\nA checksum is a calculated value which is used to check that all stored data is without error. The value is calculated according to an appropriate algorithm and stored with the data. When the data is subsequently accessed, a new checksum is calculated and compared with the original, and if they match, then no error is indicated. Checksum algorithms come in many types and versions and are recommended, and standard, practice for the detection of accidental or intentional errors in archival files.\n\nChecksums are ideal for detecting if unwanted changes to digital materials have taken place. Files should be checked against their checksums on a regular basis. How often to perform checks depends on many factors including the type of storage, how well it is maintained, and how often it is being used. As a general guideline, checking data tapes might be done annually and checking hard drive based systems might be done every six months. More frequent checks allow problems to be detected and fixed sooner, but at the expense of more load on the storage system and more processing resources.\n\nChecksums are stored in a PREMIS record inside the AIP in the storage system. There are several different checksum algorithms, e.g. MD5 and SHA-256 that can be used to generate checksums of increasing strength. The ‘stronger’ the algorithm then the harder it is to deliberately change a file in a way that goes undetected. This can be important for applications where there is a need to demonstrate resistance to malicious corruption or alteration of digital materials, for example where evidential weight and legal admissibility is important. However, if checksums are being used to detect accidental loss or damage to files, for example due to a storage failure, then MD5 is sufficient and has the advantage of being well supported in tools and is quick to calculate.",
  "identifiedOn": 1468450800000,
  "identifiedBy": "Risk automatically detected by the Risk assessment plugin \"roda.core.plugins.plugins.base.FixityPlugin\".",
  "categories": ["Data storage"],
  "notes": "Part of the description of this risk has been obtained from the Digital Preservation Coalition (DPC) Digital Preservation Handbook (http://handbook.dpconline.org/technical-solutions-and-tools/fixity-and-checksums)",
  "preMitigationProbability": 5,
  "preMitigationImpact": 4,
  "preMitigationSeverity": 20,
  "preMitigationSeverityLevel": "HIGH",
  "preMitigationNotes": "If no action is taken there is a high risk that unauthentic/corrupted data reaches the hand of end-users affecting the trustworthiness of the repository. ",
  "mitigationStrategy": "Restore file(s) from backup or storage replica and re-run the FixityPlugin to make sure that the original file has been adequately restored.\n",
  "mitigationOwnerType": "",
  "mitigationOwner": "Technical",
  "createdOn": 1468515478938,
  "createdBy": "admin"
}
